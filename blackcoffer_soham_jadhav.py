# -*- coding: utf-8 -*-
"""Blackcoffer_Soham Jadhav

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1dah4-t9kQ5hzFMsxssRR9IkrL3FQnBOd

# **Name : Soham Santosh Jadhav**
# **TY B.Tech**
# **CGPA : 9.79**
# **College: MIT-WPU**
# **email : sai.sohamiit@gmail.com**

Step 1: Importing the required libraries
"""

import pandas as pd
import nltk
import requests
from bs4 import BeautifulSoup
from nltk.corpus import stopwords
from nltk.tokenize import word_tokenize, sent_tokenize
import re
import os

"""Step 2: Load the Excel file"""

file_path = 'Input.xlsx'
sheet_name = 'Sheet1'

"""Step 3: Read the Excel file"""

df = pd.read_excel(file_path, sheet_name=sheet_name)

df.head()

urls = df.iloc[:, 1].tolist()

"""Step 4: Define a function for fetching article"""

def fetch_article(url):
    try:
        response = requests.get(url)
        response.raise_for_status()
        soup = BeautifulSoup(response.content, 'lxml')

        title = soup.find('title').get_text() if soup.find('title') else 'No title found'
        article_body = soup.find('article')

        if not article_body:
            for class_name in ['content', 'main-content', 'post-content', 'entry-content']:
                article_body = soup.find(class_=class_name)
                if article_body:
                    break

        # Extracting text
        if article_body:
            paragraphs = article_body.find_all('p')
            article_text = ' '.join([p.get_text() for p in paragraphs])
        else:
            article_text = 'No article content found'

        return {'title': title, 'text': article_text}

    except requests.RequestException as e:
        print(f"Error fetching {url}: {e}")
        return {'title': 'Error', 'text': 'Error'}

"""Step 5: Fetching articles"""

articles = [fetch_article(url) for url in urls]

"""Step 6: Display articles"""

for i, article in enumerate(articles[:3]):
    print(f"Article {i+1} Title: {article['title']}")
    print(f"Article {i+1} Text: {article['text'][:500]}")

nltk.download('punkt')
nltk.download('stopwords')

stopwords_path = r'/content/drive/MyDrive/Test Assignemnet/Blackcoffer/StopWords'
master_dict_path = r'/content/drive/MyDrive/Test Assignemnet/Blackcoffer/MasterDictionary'

"""Step 7: Set stopwords along with positive and negative words"""

stop_words = set()
for filename in os.listdir(stopwords_path):
    with open(os.path.join(stopwords_path, filename), 'r', encoding='ISO-8859-1') as file:
        stop_words.update(file.read().lower().split())

positive_words = set()
negative_words = set()
with open(os.path.join(master_dict_path, 'positive-words.txt'), 'r', encoding='ISO-8859-1') as file:
    positive_words.update(file.read().lower().split())
with open(os.path.join(master_dict_path, 'negative-words.txt'), 'r', encoding='ISO-8859-1') as file:
    negative_words.update(file.read().lower().split())

"""Step 8: Removing stopwords from positive and negative words"""

positive_words = positive_words - stop_words
negative_words = negative_words - stop_words

"""Step 9: Define functions for analysis"""

def clean_text(text):
    text = re.sub(r'[^\w\s]', '', text)
    words = word_tokenize(text.lower())
    words = [word for word in words if word not in stop_words]
    return words

def sentiment_analysis(words):
    positive_score = sum(1 for word in words if word in positive_words)
    negative_score = sum(1 for word in words if word in negative_words)
    polarity_score = (positive_score - negative_score) / ((positive_score + negative_score) + 0.000001)
    subjectivity_score = (positive_score + negative_score) / (len(words) + 0.000001)
    return positive_score, negative_score, polarity_score, subjectivity_score

def analysis_of_readability(text):
    sentences = sent_tokenize(text)
    words = word_tokenize(text)
    num_sentences = len(sentences)
    num_words = len(words)
    complex_words = [word for word in words if len([char for char in word if char in 'aeiou']) > 2]
    avg_sentence_length = num_words / num_sentences if num_sentences > 0 else 0
    percentage_complex_words = len(complex_words) / num_words if num_words > 0 else 0
    fog_index = 0.4 * (avg_sentence_length + percentage_complex_words)
    return avg_sentence_length, percentage_complex_words, fog_index

def count_complex_words(words):
    return len([word for word in words if len([char for char in word if char in 'aeiou']) > 2])

def syllable_count(word):
    word = word.lower()
    syllables = len(re.findall(r'[aeiouy]+', word))
    if word.endswith('es') or word.endswith('ed'):
        syllables = max(1, syllables - 1)
    return syllables

def text_metrics(text):
    words = clean_text(text)
    num_words = len(words)
    syllable_counts = [syllable_count(word) for word in words]
    avg_word_length = sum(len(word) for word in words) / num_words if num_words > 0 else 0
    personal_pronouns = len(re.findall(r'\b(I|we|my|ours|us)\b', text, re.I))
    return num_words, sum(syllable_counts), avg_word_length, personal_pronouns

data_output = []

for i, article in enumerate(articles):
    text = article['text']
    words = clean_text(text)
    positive_score, negative_score, polarity_score, subjectivity_score = sentiment_analysis(words)
    avg_sentence_length, percentage_complex_words, fog_index = analysis_of_readability(text)
    num_words, total_syllables, avg_word_length, personal_pronouns = text_metrics(text)

    data_output.append({
      'URL_ID': df.iloc[i, 0],
      'URL': urls[i],
      'TITLE': article['title'],
      'POSITIVE SCORE': positive_score,
      'NEGATIVE SCORE': negative_score,
      'POLARITY SCORE': polarity_score,
      'SUBJECTIVITY SCORE': subjectivity_score,
      'AVERAGE SENTENCE LENGTH': avg_sentence_length,
      'PERCENTAGE OF COMPLEX WORDS': percentage_complex_words,
      'FOG INDEX': fog_index,
      'NUMBER OF WORDS': num_words,
      'TOTAL SYLLABLES': total_syllables,
      'PERSONAL PRONOUNS': personal_pronouns,
      'AVERAGE WORD LENGTH': avg_word_length
    })

"""Step 10: Print the values of evaluation metrics"""

print(f"Positive Score: {positive_score}")
print(f"Negative Score: {negative_score}")
print(f"Polarity Score: {polarity_score}")
print(f"Subjectivity Score: {subjectivity_score}")
print(f"Average Sentence Length: {avg_sentence_length}")
print(f"Percentage of Complex Words: {percentage_complex_words}")
print(f"Fog Index: {fog_index}")
print(f"Number of Words: {num_words}")
print(f"Total Syllables: {total_syllables}")
print(f"Personal Pronouns: {personal_pronouns}")
print(f"Average Word Length: {avg_word_length}")

"""Step 11: Generate the output in Excel sheet"""

# Prepare the output DataFrame
df_output = pd.DataFrame(data_output)

# Save the output to an Excel file
output_filepath = '/content/drive/MyDrive/Test Assignemnet/Blackcoffer/Output_Analysis_Blackcoffer.xlsx'
df_output.to_excel(output_filepath, index=False)

print(f"Successfully saved to {output_filepath}")

